{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import urllib.request as urllib2\n",
    "import bs4 # beautiful soup\n",
    "import time \n",
    "import re # regular expressions\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Scraping\n",
    "Data scraping is about obtaining data from webpages. There is low level scraping where you parse the data out of the html code of the webpage. There also is scraping over APIs from websites who try to make your life a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Read the user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   user_id  age sex  occupation zip_code\n0        1   24   M  technician    85711\n1        2   53   F       other    94043\n2        3   23   M      writer    32067\n3        4   24   M  technician    43537\n4        5   33   F       other    15213",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>occupation</th>\n      <th>zip_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>24</td>\n      <td>M</td>\n      <td>technician</td>\n      <td>85711</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>53</td>\n      <td>F</td>\n      <td>other</td>\n      <td>94043</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>23</td>\n      <td>M</td>\n      <td>writer</td>\n      <td>32067</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>24</td>\n      <td>M</td>\n      <td>technician</td>\n      <td>43537</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>33</td>\n      <td>F</td>\n      <td>other</td>\n      <td>15213</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# pass in column names for each CSV\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "\n",
    "users = pd.read_csv('resources/ml-100k/u.user', sep='|',names=u_cols)\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Read the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   user_id  movie_id  rating  unix_timestamp\n0      196       242       3       881250949\n1      186       302       3       891717742\n2       22       377       1       878887116\n3      244        51       2       880606923\n4      166       346       1       886397596",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_id</th>\n      <th>rating</th>\n      <th>unix_timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>3</td>\n      <td>881250949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186</td>\n      <td>302</td>\n      <td>3</td>\n      <td>891717742</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>377</td>\n      <td>1</td>\n      <td>878887116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>244</td>\n      <td>51</td>\n      <td>2</td>\n      <td>880606923</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>166</td>\n      <td>346</td>\n      <td>1</td>\n      <td>886397596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "ratings = pd.read_csv('resources/ml-100k/u.data', sep='\\t',names=r_cols)\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now data about the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   movie_id              title release_date  video_release_date  \\\n0         1   Toy Story (1995)  01-Jan-1995                 NaN   \n1         2   GoldenEye (1995)  01-Jan-1995                 NaN   \n2         3  Four Rooms (1995)  01-Jan-1995                 NaN   \n3         4  Get Shorty (1995)  01-Jan-1995                 NaN   \n4         5     Copycat (1995)  01-Jan-1995                 NaN   \n\n                                            imdb_url  \n0  http://us.imdb.com/M/title-exact?Toy%20Story%2...  \n1  http://us.imdb.com/M/title-exact?GoldenEye%20(...  \n2  http://us.imdb.com/M/title-exact?Four%20Rooms%...  \n3  http://us.imdb.com/M/title-exact?Get%20Shorty%...  \n4  http://us.imdb.com/M/title-exact?Copycat%20(1995)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n      <th>release_date</th>\n      <th>video_release_date</th>\n      <th>imdb_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Toy Story (1995)</td>\n      <td>01-Jan-1995</td>\n      <td>NaN</td>\n      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>GoldenEye (1995)</td>\n      <td>01-Jan-1995</td>\n      <td>NaN</td>\n      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Four Rooms (1995)</td>\n      <td>01-Jan-1995</td>\n      <td>NaN</td>\n      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Get Shorty (1995)</td>\n      <td>01-Jan-1995</td>\n      <td>NaN</td>\n      <td>http://us.imdb.com/M/title-exact?Get%20Shorty%...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Copycat (1995)</td>\n      <td>01-Jan-1995</td>\n      <td>NaN</td>\n      <td>http://us.imdb.com/M/title-exact?Copycat%20(1995)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# the movies file contains columns indicating the movies's genres\n",
    "# let's only load the first 5 columns of the file with usecols\n",
    "m_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "\n",
    "movies = pd.read_csv('resources/ml-100k/u.item', sep='|',names=m_cols, usecols=range(5), encoding='latin-1')\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Get information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "movie_id                int64\ntitle                  object\nrelease_date           object\nvideo_release_date    float64\nimdb_url               object\ndtype: object\n          movie_id  video_release_date\ncount  1682.000000                 0.0\nmean    841.500000                 NaN\nstd     485.695893                 NaN\nmin       1.000000                 NaN\n25%     421.250000                 NaN\n50%     841.500000                 NaN\n75%    1261.750000                 NaN\nmax    1682.000000                 NaN\n"
    }
   ],
   "source": [
    "print(movies.dtypes)\n",
    "print\n",
    "print(movies.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Selecting data\n",
    "- DataFrame --> group of Series with shared index\n",
    "- single DataFrame column --> Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "user_id  age sex  occupation zip_code\n0        1   24   M  technician    85711\n1        2   53   F       other    94043\n2        3   23   M      writer    32067\n3        4   24   M  technician    43537\n4        5   33   F       other    15213\n----------------\n0    technician\n1         other\n2        writer\n3    technician\n4         other\nName: occupation, dtype: object\n----------------\n   occupation sex\n0  technician   M\n1       other   F\n2      writer   M\n3  technician   M\n4       other   F\n-----------------\nuser_id                4\nage                   24\nsex                    M\noccupation    technician\nzip_code           43537\nName: 3, dtype: object\n"
    }
   ],
   "source": [
    "print(users.head())\n",
    "print('----------------')\n",
    "\n",
    "print(users['occupation'].head())\n",
    "print('----------------')\n",
    "\n",
    "selected_columns = ['occupation', 'sex']\n",
    "print(users[selected_columns].head())\n",
    "print('-----------------')\n",
    "\n",
    "print(users.iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   user_id  age sex     occupation zip_code\n1        2   53   F          other    94043\n4        5   33   F          other    15213\n5        6   42   M      executive    98101\n6        7   57   M  administrator    91344\n7        8   36   M  administrator    05201",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>occupation</th>\n      <th>zip_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>53</td>\n      <td>F</td>\n      <td>other</td>\n      <td>94043</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>33</td>\n      <td>F</td>\n      <td>other</td>\n      <td>15213</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>42</td>\n      <td>M</td>\n      <td>executive</td>\n      <td>98101</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>57</td>\n      <td>M</td>\n      <td>administrator</td>\n      <td>91344</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>36</td>\n      <td>M</td>\n      <td>administrator</td>\n      <td>05201</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# select users older than 25\n",
    "old_users = users[users.age > 25]\n",
    "old_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     user_id  age sex  occupation zip_code\n18        19   40   M   librarian    02138\n82        83   40   M       other    44133\n115      116   40   M  healthcare    97232\n199      200   40   M  programmer    93402\n283      284   40   M   executive    92629",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>occupation</th>\n      <th>zip_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>40</td>\n      <td>M</td>\n      <td>librarian</td>\n      <td>02138</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>83</td>\n      <td>40</td>\n      <td>M</td>\n      <td>other</td>\n      <td>44133</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>116</td>\n      <td>40</td>\n      <td>M</td>\n      <td>healthcare</td>\n      <td>97232</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>200</td>\n      <td>40</td>\n      <td>M</td>\n      <td>programmer</td>\n      <td>93402</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>284</td>\n      <td>40</td>\n      <td>M</td>\n      <td>executive</td>\n      <td>92629</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# users aged 40 and male\n",
    "users[(users.age == 40) & (users.sex == 'M')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "user_id        age\ncount    6.000000   6.000000\nmean   411.166667  32.166667\nstd    149.987222   5.115336\nmin    292.000000  26.000000\n25%    313.000000  28.250000\n50%    378.000000  32.000000\n75%    416.750000  36.500000\nmax    698.000000  38.000000\n32.166666666666664\n32.166666666666664\n"
    }
   ],
   "source": [
    "# users who are female and programmers\n",
    "selected_users = users[(users.sex == 'F') & (users.occupation == 'programmer')]\n",
    "\n",
    "# show statistic summary\n",
    "print(selected_users.describe())\n",
    "\n",
    "# alternatives:\n",
    "print(selected_users.age.mean())\n",
    "print(selected_users['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Split-apply-combine\n",
    "- splitting the data into groups based on some criteria\n",
    "- applying a function to each group independently\n",
    "- combining the results into a data structure\n",
    "![](resources/split-apply-combine.png_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Find diligent users\n",
    "- split data per user ID\n",
    "- count ratings\n",
    "- combine result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "user_id  movie_id  rating  unix_timestamp\n0      196       242       3       881250949\n1      186       302       3       891717742\n2       22       377       1       878887116\n3      244        51       2       880606923\n4      166       346       1       886397596\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "user_id\n1    272\n2     62\n3     54\n4     24\n5    175\nName: movie_id, dtype: int64"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "print(ratings.head())\n",
    "\n",
    "# split the data\n",
    "# grouped_data = ratings.groupby('user_id)\n",
    "grouped_data = ratings['movie_id'].groupby(ratings['user_id'])\n",
    "\n",
    "# count and combine\n",
    "ratings_per_user = grouped_data.count()\n",
    "\n",
    "ratings_per_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average ratings:\nmovie_id\n1    3.878319\n2    3.206107\n3    3.033333\n4    3.550239\n5    3.302326\nName: rating, dtype: float64\n"
    }
   ],
   "source": [
    "# get the average rating per movie\n",
    "\n",
    "# split the data\n",
    "grouped_data = ratings['rating'].groupby(ratings['movie_id'])\n",
    "# average and combine\n",
    "average_ratings = grouped_data.mean()\n",
    "\n",
    "print(\"Average ratings:\")\n",
    "print(average_ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Good movie ids:\nInt64Index([814, 1122, 1189, 1201, 1293, 1467, 1500, 1536, 1599, 1653], dtype='int64', name='movie_id')\n\nBest movie titles:\n813                         Great Day in Harlem, A (1994)\n1121                       They Made Me a Criminal (1939)\n1188                                   Prefontaine (1997)\n1200           Marlene Dietrich: Shadow and Light (1996) \n1292                                      Star Kid (1997)\n1466                 Saint of Fort Washington, The (1993)\n1499                            Santa with Muscles (1996)\n1535                                 Aiqing wansui (1994)\n1598                        Someone Else's America (1995)\n1652    Entertaining Angels: The Dorothy Day Story (1996)\nName: title, dtype: object\n\n"
    }
   ],
   "source": [
    "max_rating = average_ratings.max()\n",
    "good_movie_ids = average_ratings[average_ratings == max_rating].index\n",
    "\n",
    "print(\"Good movie ids:\")\n",
    "print(good_movie_ids)\n",
    "print()\n",
    "\n",
    "print(\"Best movie titles:\")\n",
    "print(movies[movies.movie_id.isin(good_movie_ids)].title)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of ratings per movie\nmovie_id\n814     1\n1122    1\n1189    3\n1201    1\n1293    3\n1467    2\n1500    2\n1536    1\n1599    1\n1653    1\nName: rating, dtype: int64\n"
    }
   ],
   "source": [
    "how_many_ratings = grouped_data.count()\n",
    "print(\"Number of ratings per movie\")\n",
    "print(how_many_ratings[average_ratings == max_rating])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passing a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "movie_id\n1    3.878319\n2    3.206107\n3    3.033333\n4    3.550239\n5    3.302326\nName: rating, dtype: float64"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "average_ratings = grouped_data.apply(lambda f: f.mean())\n",
    "average_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "user_id\n1    3.610294\n2    3.709677\n3    2.796296\n4    4.333333\n5    2.874286\nName: rating, dtype: float64"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# get the average rating per user\n",
    "grouped_data = ratings['rating'].groupby(ratings['user_id'])\n",
    "average_ratings = grouped_data.mean()\n",
    "average_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "occupation\nadministrator     True\nartist            True\ndoctor            True\neducator          True\nengineer          True\nentertainment     True\nexecutive         True\nhealthcare       False\nhomemaker        False\nlawyer            True\nlibrarian        False\nmarketing         True\nnone              True\nother             True\nprogrammer        True\nretired           True\nsalesman          True\nscientist         True\nstudent           True\ntechnician        True\nwriter            True\nName: sex, dtype: bool\n\n\n"
    }
   ],
   "source": [
    "# list all occupations and if they are male or female dominant\n",
    "grouped_data = users['sex'].groupby(users['occupation'])\n",
    "male_dominant_occupations = grouped_data.apply(lambda f: sum(f == 'M') > sum(f == 'F'))\n",
    "\n",
    "print(male_dominant_occupations)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of male users:\n670\nnumber of female users\n273\n"
    }
   ],
   "source": [
    "print(\"number of male users:\")\n",
    "print(sum(users['sex'] == 'M'))\n",
    "\n",
    "print(\"number of female users\")\n",
    "print(sum(users['sex'] == 'F'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python data scraping\n",
    "Why scrape the web?\n",
    "- vast source of information\n",
    "- automate tasks\n",
    "- keep up with sites\n",
    "Some examples:\n",
    "- stock market monitoring\n",
    "- sports data\n",
    "- airline prices\n",
    "- amazon, twitter, indeed, rikunabi, mynabi, etc...\n",
    "\n",
    "Copyrights and permission:\n",
    "- be careful and polite\n",
    "- give credit \n",
    "- care about media law\n",
    "- no spam, overloading sites, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Robots.txt\n",
    "- specified by web site owner\n",
    "- gives instructions to web robots(your script)\n",
    "- is located at the top-level directory of the web server\n",
    "\n",
    "For example:\n",
    "\n",
    "[http://www.example.com/robots.txt](http://www.example.com/robots.txt)\n",
    "\n",
    "[http://google.com/robots.txt](http://google.com/robots.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scraping with Python\n",
    "- scraping is all about HTML tags\n",
    "- bad news:\n",
    "    - need to learn about tags\n",
    "    - websites can be very ugly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HTML\n",
    "- HyperTextMarkup Language\n",
    "- standard for creating webpages\n",
    "- HTML tags\n",
    "    - have angle barckets\n",
    "    - typically come in pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<html>\n    <head>\n        <title>This is a title</title>\n    </head>\n    <body>\n        <h2>Test</h2>\n        <p>Hello world!</p>\n    </body>\n</html>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "htmlString = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>This is a title</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>Test</h2>\n",
    "        <p>Hello world!</p>\n",
    "    </body>\n",
    "</html>\"\"\"\n",
    "\n",
    "htmlOutput = HTML(htmlString)\n",
    "htmlOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful tags\n",
    "- heading `<h1></h1> ... <h6></h6>`\n",
    "- paragraph `<p></p>`\n",
    "- line break `<br>`\n",
    "- link with attribute `<a href=\"http://www.example.com/\">An example link</a>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scraping with Python\n",
    "- example of a beautiful simple webpage:\n",
    "[http://www.crummy.com/software/BeautifulSoup](http://www.crummy.com/software/BeautifulSoup)\n",
    "\n",
    "- different useful libraries:\n",
    "    - urllib\n",
    "    - beautifulsoup \n",
    "    - pattern\n",
    "    - soupy \n",
    "    - LXML\n",
    "    - ...\n",
    "\n",
    "The following cell defines a url as a string and then reads the data from that url using the `urllib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://www.crummy.com/software/BeautifulSoup'\n",
    "source = urllib2.urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In python 3, 'strings' are by default 'bytecodes' not 'unicode'.\n",
    "\n",
    "This means that the string is a byte string and not unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we print we see that we got the whole HTML content of the page into the string variable `source`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/nb/themes/Default/nb.css\">\n<meta name=\"Description\" content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\">\n<meta name=\"generator\" content=\"Markov Approximation 1.4 (module: leonardr)\">\n<meta name=\"author\" content=\"Leonard Richardson\">\n</head>\n<body bgcolor=\"white\" text=\"black\" link=\"blue\" vlink=\"660066\" alink=\"red\">\n<style>\n#tidelift { }\n\n#tidelift a {\n border: 1px solid #666666;\n margin-left: auto;\n padding: 10px;\n text-decoration: none;\n}\n\n#tidelift .cta {\n background: url(\"tidelift.svg\") no-repeat;\n padding-left: 30px;\n}\n</style>\t\t   \n\n<img align=\"right\" src=\"10.1.jpg\" width=\"250\"><br />\n\n<p>[ <a href=\"#Download\">Download</a> | <a\nhref=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"enterprise.html\">For enterprise</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\n\n<div align=\"center\">\n\n<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\n\n</div>\n\n<p>You didn't write that awful page. You're just trying to get some\ndata out of it. Beautiful Soup is here to help. Since 2004, it's been\nsaving programmers hours or days of work on quick-turnaround\nscreen scraping projects.</p>\n\n<p>Beautiful Soup is a Python library designed for quick turnaround\nprojects like screen-scraping. Three features make it powerful:\n\n<ol>\n\n<li>Beautiful Soup provides a few simple methods and Pythonic idioms\nfor navigating, searching, and modifying a parse tree: a toolkit for\ndissecting a document and extracting what you need. It doesn't take\nmuch code to write an application\n\n<li>Beautiful Soup automatically converts incoming documents to\nUnicode and outgoing documents to UTF-8. You don't have to think\nabout encodings, unless the document doesn't specify an encoding and\nBeautiful Soup can't detect one. Then you just have to specify the\noriginal encoding.\n\n<li>Beautiful Soup sits on top of popular Python parsers like <a\nhref=\"http://lxml.de/\">lxml</a> and <a\nhref=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\nto try out different parsing strategies or trade speed for\nflexibility.\n\n</ol>\n\n<p>Beautiful Soup parses anything you give it, and does the tree\ntraversal stuff for you. You can tell it \"Find all the links\", or\n\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\nlinks whose urls match \"foo.com\", or \"Find the table heading that's\ngot bold text, then give me that text.\"\n\n<p>Valuable data that was once locked up in poorly-designed websites\nis now within your reach. Projects that would have taken hours take\nonly minutes with Beautiful Soup.\n\n<p>Interested? <a href=\"bs4/doc/\">Read more.</a>\n\n<h3>Getting and giving support</h3>\n\n<div id=\"tidelift\" align=\"center\">\n<a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise\" target=\"_blank\">\n <span class=\"cta\">\n  Beautiful Soup for enterprise available via Tidelift\n </span>\n</a>\n</div>\n\n<p>If you have questions, send them to <a\nhref=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it on Launchpad</a>. If it's a security vulnerability, report it confidentially through <a href=\"https://tidelift.com/security\">Tidelift</a>.</p>\n\n<p>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n\n\n<p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p>\n</div>\n\n\n<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\n\n<p>The current release is <a href=\"bs4/download/\">Beautiful Soup\n4.9.0</a> (April 5, 2020). You can install Beautiful Soup 4 with\n<code>pip install beautifulsoup4</code>.\n\n<p>In Debian and Ubuntu, Beautiful Soup is available as the\n<code>python-bs4</code> package (for Python 2) or the\n<code>python3-bs4</code> package (for Python 3). In Fedora it's\navailable as the <code>python-beautifulsoup4</code> package.\n\n<p>Beautiful Soup is licensed under the MIT license, so you can also\ndownload the tarball, drop the <code>bs4/</code> directory into almost\nany Python application (or into your library path) and start using it\nimmediately. (If you want to do this under Python 3, you will need to\nmanually convert the code using <code>2to3</code>.)\n\n<p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python\n3. Support for Python 2 will be discontinued on or after December 31,\n2020&mdash;one year after the Python 2 sunsetting date.\n\n<h3>Beautiful Soup 3</h3>\n\n<p>Beautiful Soup 3 was the official release line of Beautiful Soup\nfrom May 2006 to March 2012. It does not support Python 3 and it will\nbe discontinued on or after December 31, 2020&mdash;one year after the\nPython 2 sunsetting date. If you have any active projects using\nBeautiful Soup 3, you should migrate to Beautiful Soup 4 as part of\nyour Python 3 conversion.\n\n<p><a\nhref=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here's\nthe Beautiful Soup 3 documentation.</a>\n\n<p>The current and hopefully final release of Beautiful Soup 3 is <a\nhref=\"download/3.x/BeautifulSoup-3.2.2.tar.gz\">3.2.2</a> (October 5,\n2019). It's the <code>BeautifulSoup</code> package on pip. It's also\navailable as <code>python-beautifulsoup</code> in Debian and Ubuntu,\nand as <code>python-BeautifulSoup</code> in Fedora.\n\n<p>Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n\n<p>Beautiful Soup 3, like Beautiful Soup 4, is <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website\">supported through Tidelift</a>.</p>\n\n<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\n\n<p>Over the years, Beautiful Soup has been used in hundreds of\ndifferent projects. There's no way I can list them all, but I want to\nhighlight a few high-profile projects. Beautiful Soup isn't what makes\nthese projects interesting, but it did make their completion easier:\n\n<ul>\n\n<li><a\n href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n Type\"</a>, a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n\n<li>Jiabao Lin's <a\nhref=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">DXY-COVID-19-Crawler</a>\nuses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source: <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\"How open source software is fighting COVID-19\"</a>)\n\n<li>Reddit uses Beautiful Soup to <a\nhref=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\na page that's been linked to and find a representative image</a>.\n\n<li>Alexander Harrowell uses Beautiful Soup to <a\n href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n activities</a> of an arms merchant.\n\n<li>The developers of Python itself used Beautiful Soup to <a\nhref=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\nbug tracker from Sourceforge to Roundup</a>.\n\n<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\nuses Beautiful Soup to <A\nhref=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\nstatewide election results</a>.\n\n<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\nApplications Branch</a> uses Beautiful Soup in <a\nhref=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\ndownloading \"high resolution USGS datasets.\"\n\n</ul>\n\n<p>If you've used Beautiful Soup in a project you'd like me to know\nabout, please do send email to me or <a\nhref=\"http://groups.google.com/group/beautifulsoup/\">the discussion\ngroup</a>.\n\n<h2>Development</h2>\n\n<p>Development happens at <a\nhref=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a\nhref=\"https://code.launchpad.net/beautifulsoup/\">get the source\ncode</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\nbugs</a>.<hr><table><tr><td valign=\"top\">\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, April 06 2020, 17:23:04 Nowhere Standard Time and last built on Tuesday, April 21 2020, 10:00:01 Nowhere Standard Time.</p><p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"></a></td><td valign=\"top\">Crummy is &copy; 1996-2020 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table></span><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></p></td><td valign=top><p><b>Document tree:</b>\n<dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dl>\n</dl>\n</dl>\n\n\nSite Search:\n\n<form method=\"get\" action=\"/search/\">\n        <input type=\"text\" name=\"q\" maxlength=\"255\" value=\"\"></input>\n        </form>\n        </td>\n\n</tr>\n\n</table>\n</body>\n</html>\n\n"
    }
   ],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions:\n",
    "- Is the word 'Alice' mentioned on the beautiful soup homepage?\n",
    "- How often does the word 'Soup' occur on the site?\n",
    "    - hint: use `.count()`\n",
    "- At what index occurs the substring 'alien video games'?\n",
    "    - hint: use `.find()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\n"
    }
   ],
   "source": [
    "# is 'Alice' in source?\n",
    "print('Alice' in source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "50\n"
    }
   ],
   "source": [
    "# count occurences of 'Soup'\n",
    "print(source.count('Soup'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-1\n"
    }
   ],
   "source": [
    "# find index of 'alien video games'\n",
    "position = source.find('alien video games')\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "203\n"
    }
   ],
   "source": [
    "# find index of 'alien video games'\n",
    "position = source.find('Beautiful Soup')\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Beautiful Soup: We c\n"
    }
   ],
   "source": [
    "# quick test to see the substring in the source variable\n",
    "# you can access strings like lists\n",
    "print(source[position:position+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Beautiful Soup\n"
    }
   ],
   "source": [
    "# tidier version\n",
    "print(source[position:position+len('Beautiful Soup')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beautiful Soup\n",
    "- designed to make your life easier\n",
    "- many good functions for parsing html code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# get bs4 object\n",
    "soup = bs4.BeautifulSoup(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n\n<html>\n<head>\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n<link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n<link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n<meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n<meta content=\"Leonard Richardson\" name=\"author\"/>\n</head>\n<body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n<style>\n#tidelift { }\n\n#tidelift a {\n border: 1px solid #666666;\n margin-left: auto;\n padding: 10px;\n text-decoration: none;\n}\n\n#tidelift .cta {\n background: url(\"tidelift.svg\") no-repeat;\n padding-left: 30px;\n}\n</style>\n<img align=\"right\" src=\"10.1.jpg\" width=\"250\"/><br/>\n<p>[ <a href=\"#Download\">Download</a> | <a href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"enterprise.html\">For enterprise</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\n<div align=\"center\">\n<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\n</div>\n<p>You didn't write that awful page. You're just trying to get some\ndata out of it. Beautiful Soup is here to help. Since 2004, it's been\nsaving programmers hours or days of work on quick-turnaround\nscreen scraping projects.</p>\n<p>Beautiful Soup is a Python library designed for quick turnaround\nprojects like screen-scraping. Three features make it powerful:\n\n<ol>\n<li>Beautiful Soup provides a few simple methods and Pythonic idioms\nfor navigating, searching, and modifying a parse tree: a toolkit for\ndissecting a document and extracting what you need. It doesn't take\nmuch code to write an application\n\n<li>Beautiful Soup automatically converts incoming documents to\nUnicode and outgoing documents to UTF-8. You don't have to think\nabout encodings, unless the document doesn't specify an encoding and\nBeautiful Soup can't detect one. Then you just have to specify the\noriginal encoding.\n\n<li>Beautiful Soup sits on top of popular Python parsers like <a href=\"http://lxml.de/\">lxml</a> and <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\nto try out different parsing strategies or trade speed for\nflexibility.\n\n</li></li></li></ol>\n<p>Beautiful Soup parses anything you give it, and does the tree\ntraversal stuff for you. You can tell it \"Find all the links\", or\n\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\nlinks whose urls match \"foo.com\", or \"Find the table heading that's\ngot bold text, then give me that text.\"\n\n<p>Valuable data that was once locked up in poorly-designed websites\nis now within your reach. Projects that would have taken hours take\nonly minutes with Beautiful Soup.\n\n<p>Interested? <a href=\"bs4/doc/\">Read more.</a>\n<h3>Getting and giving support</h3>\n<div align=\"center\" id=\"tidelift\">\n<a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=enterprise\" target=\"_blank\">\n<span class=\"cta\">\n  Beautiful Soup for enterprise available via Tidelift\n </span>\n</a>\n</div>\n<p>If you have questions, send them to <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it on Launchpad</a>. If it's a security vulnerability, report it confidentially through <a href=\"https://tidelift.com/security\">Tidelift</a>.</p>\n<p>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n\n\n<p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p>\n</p></p></p></p></p></body></html>\n<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\n<p>The current release is <a href=\"bs4/download/\">Beautiful Soup\n4.9.0</a> (April 5, 2020). You can install Beautiful Soup 4 with\n<code>pip install beautifulsoup4</code>.\n\n<p>In Debian and Ubuntu, Beautiful Soup is available as the\n<code>python-bs4</code> package (for Python 2) or the\n<code>python3-bs4</code> package (for Python 3). In Fedora it's\navailable as the <code>python-beautifulsoup4</code> package.\n\n<p>Beautiful Soup is licensed under the MIT license, so you can also\ndownload the tarball, drop the <code>bs4/</code> directory into almost\nany Python application (or into your library path) and start using it\nimmediately. (If you want to do this under Python 3, you will need to\nmanually convert the code using <code>2to3</code>.)\n\n<p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python\n3. Support for Python 2 will be discontinued on or after December 31,\n2020—one year after the Python 2 sunsetting date.\n\n<h3>Beautiful Soup 3</h3>\n<p>Beautiful Soup 3 was the official release line of Beautiful Soup\nfrom May 2006 to March 2012. It does not support Python 3 and it will\nbe discontinued on or after December 31, 2020—one year after the\nPython 2 sunsetting date. If you have any active projects using\nBeautiful Soup 3, you should migrate to Beautiful Soup 4 as part of\nyour Python 3 conversion.\n\n<p><a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here's\nthe Beautiful Soup 3 documentation.</a>\n<p>The current and hopefully final release of Beautiful Soup 3 is <a href=\"download/3.x/BeautifulSoup-3.2.2.tar.gz\">3.2.2</a> (October 5,\n2019). It's the <code>BeautifulSoup</code> package on pip. It's also\navailable as <code>python-beautifulsoup</code> in Debian and Ubuntu,\nand as <code>python-BeautifulSoup</code> in Fedora.\n\n<p>Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n\n<p>Beautiful Soup 3, like Beautiful Soup 4, is <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&amp;utm_medium=referral&amp;utm_campaign=website\">supported through Tidelift</a>.</p>\n<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\n<p>Over the years, Beautiful Soup has been used in hundreds of\ndifferent projects. There's no way I can list them all, but I want to\nhighlight a few high-profile projects. Beautiful Soup isn't what makes\nthese projects interesting, but it did make their completion easier:\n\n<ul>\n<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n Type\"</a>, a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n\n<li>Jiabao Lin's <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">DXY-COVID-19-Crawler</a>\nuses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source: <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\"How open source software is fighting COVID-19\"</a>)\n\n<li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\na page that's been linked to and find a representative image</a>.\n\n<li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n activities</a> of an arms merchant.\n\n<li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\nbug tracker from Sourceforge to Roundup</a>.\n\n<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\nstatewide election results</a>.\n\n<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\ndownloading \"high resolution USGS datasets.\"\n\n</li></li></li></li></li></li></li></ul>\n<p>If you've used Beautiful Soup in a project you'd like me to know\nabout, please do send email to me or <a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\ngroup</a>.\n\n<h2>Development</h2>\n<p>Development happens at <a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\ncode</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\nbugs</a>.<hr/><table><tr><td valign=\"top\">\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, April 06 2020, 17:23:04 Nowhere Standard Time and last built on Tuesday, April 21 2020, 10:00:01 Nowhere Standard Time.</p><p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2020 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table></p></td></tr></table></p></p></p></p></p></p></p></p></p></p></p><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--><td valign=\"top\"><p><b>Document tree:</b>\n<dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\n</dd></dl>\n</dd></dl>\n\n\nSite Search:\n\n<form action=\"/search/\" method=\"get\">\n<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n</form>\n</p></td>\n\n\n\n\n\n"
    }
   ],
   "source": [
    "# compare the two print statements\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n<html>\n <head>\n  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n  <title>\n   Beautiful Soup: We called him Tortoise because he taught us.\n  </title>\n  <link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n  <link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n  <meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n  <meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n  <meta content=\"Leonard Richardson\" name=\"author\"/>\n </head>\n <body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n  <style>\n   #tidelift { }\n\n#tidelift a {\n border: 1px solid #666666;\n margin-left: auto;\n padding: 10px;\n text-decoration: none;\n}\n\n#tidelift .cta {\n background: url(\"tidelift.svg\") no-repeat;\n padding-left: 30px;\n}\n  </style>\n  <img align=\"right\" src=\"10.1.jpg\" width=\"250\"/>\n  <br/>\n  <p>\n   [\n   <a href=\"#Download\">\n    Download\n   </a>\n   |\n   <a href=\"bs4/doc/\">\n    Documentation\n   </a>\n   |\n   <a href=\"#HallOfFame\">\n    Hall of Fame\n   </a>\n   |\n   <a href=\"enterprise.html\">\n    For enterprise\n   </a>\n   |\n   <a href=\"https://code.launchpad.net/beautifulsoup\">\n    Source\n   </a>\n   |\n   <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">\n    Changelog\n   </a>\n   |\n   <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n    Discussion group\n   </a>\n   |\n   <a href=\"zine/\">\n    Zine\n   </a>\n   ]\n  </p>\n  <div align=\"center\">\n   <a href=\"bs4/download/\">\n    <h1>\n     Beautiful Soup\n    </h1>\n   </a>\n  </div>\n  <p>\n   You didn't write that awful page. You're just trying to get some\ndata out of it. Beautiful Soup is here to help. Since 2004, it's been\nsaving programmers hours or days of work on quick-turnaround\nscreen scraping projects.\n  </p>\n  <p>\n   Beautiful Soup is a Python library designed for quick turnaround\nprojects like screen-scraping. Three features make it powerful:\n   <ol>\n    <li>\n     Beautiful Soup provides a few simple methods and Pythonic idioms\nfor navigating, searching, and modifying a parse tree: a toolkit for\ndissecting a document and extracting what you need. It doesn't take\nmuch code to write an application\n     <li>\n      Beautiful Soup automatically converts incoming documents to\nUnicode and outgoing documents to UTF-8. You don't have to think\nabout encodings, unless the document doesn't specify an encoding and\nBeautiful Soup can't detect one. Then you just have to specify the\noriginal encoding.\n      <li>\n       Beautiful Soup sits on top of popular Python parsers like\n       <a href=\"http://lxml.de/\">\n        lxml\n       </a>\n       and\n       <a href=\"http://code.google.com/p/html5lib/\">\n        html5lib\n       </a>\n       , allowing you\nto try out different parsing strategies or trade speed for\nflexibility.\n      </li>\n     </li>\n    </li>\n   </ol>\n   <p>\n    Beautiful Soup parses anything you give it, and does the tree\ntraversal stuff for you. You can tell it \"Find all the links\", or\n\"Find all the links of class\n    <tt>\n     externalLink\n    </tt>\n    \", or \"Find all the\nlinks whose urls match \"foo.com\", or \"Find the table heading that's\ngot bold text, then give me that text.\"\n    <p>\n     Valuable data that was once locked up in poorly-designed websites\nis now within your reach. Projects that would have taken hours take\nonly minutes with Beautiful Soup.\n     <p>\n      Interested?\n      <a href=\"bs4/doc/\">\n       Read more.\n      </a>\n      <h3>\n       Getting and giving support\n      </h3>\n      <div align=\"center\" id=\"tidelift\">\n       <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=enterprise\" target=\"_blank\">\n        <span class=\"cta\">\n         Beautiful Soup for enterprise available via Tidelift\n        </span>\n       </a>\n      </div>\n      <p>\n       If you have questions, send them to\n       <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n        the discussion\ngroup\n       </a>\n       . If you find a bug,\n       <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n        file it on Launchpad\n       </a>\n       . If it's a security vulnerability, report it confidentially through\n       <a href=\"https://tidelift.com/security\">\n        Tidelift\n       </a>\n       .\n      </p>\n      <p>\n       If you use Beautiful Soup as part of your work, please consider a\n       <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">\n        Tidelift subscription\n       </a>\n       . This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n       <p>\n        If Beautiful Soup is useful to you on a personal level, you might like to read\n        <a href=\"zine/\">\n         <i>\n          Tool Safety\n         </i>\n        </a>\n        , a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n       </p>\n      </p>\n     </p>\n    </p>\n   </p>\n  </p>\n </body>\n</html>\n<a name=\"Download\">\n <h2>\n  Download Beautiful Soup\n </h2>\n</a>\n<p>\n The current release is\n <a href=\"bs4/download/\">\n  Beautiful Soup\n4.9.0\n </a>\n (April 5, 2020). You can install Beautiful Soup 4 with\n <code>\n  pip install beautifulsoup4\n </code>\n .\n <p>\n  In Debian and Ubuntu, Beautiful Soup is available as the\n  <code>\n   python-bs4\n  </code>\n  package (for Python 2) or the\n  <code>\n   python3-bs4\n  </code>\n  package (for Python 3). In Fedora it's\navailable as the\n  <code>\n   python-beautifulsoup4\n  </code>\n  package.\n  <p>\n   Beautiful Soup is licensed under the MIT license, so you can also\ndownload the tarball, drop the\n   <code>\n    bs4/\n   </code>\n   directory into almost\nany Python application (or into your library path) and start using it\nimmediately. (If you want to do this under Python 3, you will need to\nmanually convert the code using\n   <code>\n    2to3\n   </code>\n   .)\n   <p>\n    Beautiful Soup 4 works on both Python 2 (2.7+) and Python\n3. Support for Python 2 will be discontinued on or after December 31,\n2020—one year after the Python 2 sunsetting date.\n    <h3>\n     Beautiful Soup 3\n    </h3>\n    <p>\n     Beautiful Soup 3 was the official release line of Beautiful Soup\nfrom May 2006 to March 2012. It does not support Python 3 and it will\nbe discontinued on or after December 31, 2020—one year after the\nPython 2 sunsetting date. If you have any active projects using\nBeautiful Soup 3, you should migrate to Beautiful Soup 4 as part of\nyour Python 3 conversion.\n     <p>\n      <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">\n       Here's\nthe Beautiful Soup 3 documentation.\n      </a>\n      <p>\n       The current and hopefully final release of Beautiful Soup 3 is\n       <a href=\"download/3.x/BeautifulSoup-3.2.2.tar.gz\">\n        3.2.2\n       </a>\n       (October 5,\n2019). It's the\n       <code>\n        BeautifulSoup\n       </code>\n       package on pip. It's also\navailable as\n       <code>\n        python-beautifulsoup\n       </code>\n       in Debian and Ubuntu,\nand as\n       <code>\n        python-BeautifulSoup\n       </code>\n       in Fedora.\n       <p>\n        Once Beautiful Soup 3 is discontinued, these package names will be available for use by a more recent version of Beautiful Soup.\n        <p>\n         Beautiful Soup 3, like Beautiful Soup 4, is\n         <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&amp;utm_medium=referral&amp;utm_campaign=website\">\n          supported through Tidelift\n         </a>\n         .\n        </p>\n        <a name=\"HallOfFame\">\n         <h2>\n          Hall of Fame\n         </h2>\n        </a>\n        <p>\n         Over the years, Beautiful Soup has been used in hundreds of\ndifferent projects. There's no way I can list them all, but I want to\nhighlight a few high-profile projects. Beautiful Soup isn't what makes\nthese projects interesting, but it did make their completion easier:\n         <ul>\n          <li>\n           <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\n            \"Movable\n Type\"\n           </a>\n           , a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n           <li>\n            Jiabao Lin's\n            <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">\n             DXY-COVID-19-Crawler\n            </a>\n            uses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source:\n            <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\n             \"How open source software is fighting COVID-19\"\n            </a>\n            )\n            <li>\n             Reddit uses Beautiful Soup to\n             <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">\n              parse\na page that's been linked to and find a representative image\n             </a>\n             .\n             <li>\n              Alexander Harrowell uses Beautiful Soup to\n              <a href=\"http://www.harrowell.org.uk/viktormap.html\">\n               track the business\n activities\n              </a>\n              of an arms merchant.\n              <li>\n               The developers of Python itself used Beautiful Soup to\n               <a href=\"http://svn.python.org/view/tracker/importer/\">\n                migrate the Python\nbug tracker from Sourceforge to Roundup\n               </a>\n               .\n               <li>\n                The\n                <a href=\"http://www2.ljworld.com/\">\n                 Lawrence Journal-World\n                </a>\n                uses Beautiful Soup to\n                <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">\n                 gather\nstatewide election results\n                </a>\n                .\n                <li>\n                 The\n                 <a href=\"http://esrl.noaa.gov/gsd/fab/\">\n                  NOAA's Forecast\nApplications Branch\n                 </a>\n                 uses Beautiful Soup in\n                 <a href=\"http://laps.noaa.gov/topograbber/\">\n                  TopoGrabber\n                 </a>\n                 , a script for\ndownloading \"high resolution USGS datasets.\"\n                </li>\n               </li>\n              </li>\n             </li>\n            </li>\n           </li>\n          </li>\n         </ul>\n         <p>\n          If you've used Beautiful Soup in a project you'd like me to know\nabout, please do send email to me or\n          <a href=\"http://groups.google.com/group/beautifulsoup/\">\n           the discussion\ngroup\n          </a>\n          .\n          <h2>\n           Development\n          </h2>\n          <p>\n           Development happens at\n           <a href=\"https://launchpad.net/beautifulsoup\">\n            Launchpad\n           </a>\n           . You can\n           <a href=\"https://code.launchpad.net/beautifulsoup/\">\n            get the source\ncode\n           </a>\n           or\n           <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n            file\nbugs\n           </a>\n           .\n           <hr/>\n           <table>\n            <tr>\n             <td valign=\"top\">\n              <p>\n               This document (\n               <a href=\"/source/software/BeautifulSoup/index.bhtml\">\n                source\n               </a>\n               ) is part of Crummy, the webspace of\n               <a href=\"/self/\">\n                Leonard Richardson\n               </a>\n               (\n               <a href=\"/self/contact.html\">\n                contact information\n               </a>\n               ). It was last modified on Monday, April 06 2020, 17:23:04 Nowhere Standard Time and last built on Tuesday, April 21 2020, 10:00:01 Nowhere Standard Time.\n              </p>\n              <p>\n               <table class=\"licenseText\">\n                <tr>\n                 <td>\n                  <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n                   <img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/>\n                  </a>\n                 </td>\n                 <td valign=\"top\">\n                  Crummy is © 1996-2020 Leonard Richardson. Unless otherwise noted, all text licensed under a\n                  <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n                   Creative Commons License\n                  </a>\n                  .\n                 </td>\n                </tr>\n               </table>\n              </p>\n             </td>\n            </tr>\n           </table>\n          </p>\n         </p>\n        </p>\n       </p>\n      </p>\n     </p>\n    </p>\n   </p>\n  </p>\n </p>\n</p>\n<!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>-->\n<td valign=\"top\">\n <p>\n  <b>\n   Document tree:\n  </b>\n  <dl>\n   <dd>\n    <a href=\"http://www.crummy.com/\">\n     http://www.crummy.com/\n    </a>\n    <dl>\n     <dd>\n      <a href=\"http://www.crummy.com/software/\">\n       software/\n      </a>\n      <dl>\n       <dd>\n        <a href=\"http://www.crummy.com/software/BeautifulSoup/\">\n         BeautifulSoup/\n        </a>\n       </dd>\n      </dl>\n     </dd>\n    </dl>\n   </dd>\n  </dl>\n  Site Search:\n  <form action=\"/search/\" method=\"get\">\n   <input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n  </form>\n </p>\n</td>\n\n"
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<a href=\"#Download\">Download</a>,\n <a href=\"bs4/doc/\">Documentation</a>,\n <a href=\"#HallOfFame\">Hall of Fame</a>,\n <a href=\"enterprise.html\">For enterprise</a>,\n <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a>,\n <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a>,\n <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>,\n <a href=\"zine/\">Zine</a>,\n <a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>,\n <a href=\"http://lxml.de/\">lxml</a>,\n <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>,\n <a href=\"bs4/doc/\">Read more.</a>,\n <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=enterprise\" target=\"_blank\">\n <span class=\"cta\">\n   Beautiful Soup for enterprise available via Tidelift\n  </span>\n </a>,\n <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\n group</a>,\n <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it on Launchpad</a>,\n <a href=\"https://tidelift.com/security\">Tidelift</a>,\n <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>,\n <a href=\"zine/\"><i>Tool Safety</i></a>,\n <a name=\"Download\"><h2>Download Beautiful Soup</h2></a>,\n <a href=\"bs4/download/\">Beautiful Soup\n 4.9.0</a>,\n <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here's\n the Beautiful Soup 3 documentation.</a>,\n <a href=\"download/3.x/BeautifulSoup-3.2.2.tar.gz\">3.2.2</a>,\n <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&amp;utm_medium=referral&amp;utm_campaign=website\">supported through Tidelift</a>,\n <a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>,\n <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n  Type\"</a>,\n <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">DXY-COVID-19-Crawler</a>,\n <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\"How open source software is fighting COVID-19\"</a>,\n <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\n a page that's been linked to and find a representative image</a>,\n <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n  activities</a>,\n <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\n bug tracker from Sourceforge to Roundup</a>,\n <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>,\n <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\n statewide election results</a>,\n <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\n Applications Branch</a>,\n <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>,\n <a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\n group</a>,\n <a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>,\n <a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\n code</a>,\n <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\n bugs</a>,\n <a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>,\n <a href=\"/self/\">Leonard Richardson</a>,\n <a href=\"/self/contact.html\">contact information</a>,\n <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a>,\n <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>,\n <a href=\"http://www.crummy.com/\">http://www.crummy.com/</a>,\n <a href=\"http://www.crummy.com/software/\">software/</a>,\n <a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a>]"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# show how to find all 'a' tags\n",
    "soup.findAll('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why does this not work?\n",
    "\n",
    "```python\n",
    "soup.findAll('Soup')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<a href=\"#Download\">Download</a>\n"
    }
   ],
   "source": [
    "# get attribute value from an element\n",
    "# find tag: this only returns the first occurence, not all tags in the string\n",
    "first_tag = soup.find('a')\n",
    "print(first_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'#Download'"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# get attribute 'href'\n",
    "first_tag.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['#Download', 'bs4/doc/', '#HallOfFame', 'enterprise.html', 'https://code.launchpad.net/beautifulsoup', 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG', 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup', 'zine/', 'bs4/download/', 'http://lxml.de/', 'http://code.google.com/p/html5lib/', 'bs4/doc/', 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise', 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup', 'https://bugs.launchpad.net/beautifulsoup/', 'https://tidelift.com/security', 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website', 'zine/', None, 'bs4/download/', 'http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html', 'download/3.x/BeautifulSoup-3.2.2.tar.gz', 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website', None, 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html', 'https://github.com/BlankerL/DXY-COVID-19-Crawler', 'https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19', 'https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py', 'http://www.harrowell.org.uk/viktormap.html', 'http://svn.python.org/view/tracker/importer/', 'http://www2.ljworld.com/', 'http://www.b-list.org/weblog/2010/nov/02/news-done-broke/', 'http://esrl.noaa.gov/gsd/fab/', 'http://laps.noaa.gov/topograbber/', 'http://groups.google.com/group/beautifulsoup/', 'https://launchpad.net/beautifulsoup', 'https://code.launchpad.net/beautifulsoup/', 'https://bugs.launchpad.net/beautifulsoup/', '/source/software/BeautifulSoup/index.bhtml', '/self/', '/self/contact.html', 'http://creativecommons.org/licenses/by-sa/2.0/', 'http://creativecommons.org/licenses/by-sa/2.0/', 'http://www.crummy.com/', 'http://www.crummy.com/software/', 'http://www.crummy.com/software/BeautifulSoup/']\n"
    }
   ],
   "source": [
    "# get all links in the page\n",
    "link_list = [l.get('href') for l in soup.findAll('a')]\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-fbba2a637cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# if it starts with 'http', it is ok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'http'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mexternal_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# filter all external links\n",
    "# create an empty list to collect the valid links\n",
    "external_links = []\n",
    "\n",
    "# write a loop to filter the links \n",
    "# if it starts with 'http', it is ok\n",
    "for l in link_list:\n",
    "    if l[:4] == 'http':\n",
    "        external_links.append(l)\n",
    "\n",
    "# this throws an error! it says something about 'NoneType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['#Download',\n 'bs4/doc/',\n '#HallOfFame',\n 'enterprise.html',\n 'https://code.launchpad.net/beautifulsoup',\n 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n 'zine/',\n 'bs4/download/',\n 'http://lxml.de/',\n 'http://code.google.com/p/html5lib/',\n 'bs4/doc/',\n 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise',\n 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n 'https://bugs.launchpad.net/beautifulsoup/',\n 'https://tidelift.com/security',\n 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n 'zine/',\n None,\n 'bs4/download/',\n 'http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html',\n 'download/3.x/BeautifulSoup-3.2.2.tar.gz',\n 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website',\n None,\n 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n 'https://github.com/BlankerL/DXY-COVID-19-Crawler',\n 'https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19',\n 'https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py',\n 'http://www.harrowell.org.uk/viktormap.html',\n 'http://svn.python.org/view/tracker/importer/',\n 'http://www2.ljworld.com/',\n 'http://www.b-list.org/weblog/2010/nov/02/news-done-broke/',\n 'http://esrl.noaa.gov/gsd/fab/',\n 'http://laps.noaa.gov/topograbber/',\n 'http://groups.google.com/group/beautifulsoup/',\n 'https://launchpad.net/beautifulsoup',\n 'https://code.launchpad.net/beautifulsoup/',\n 'https://bugs.launchpad.net/beautifulsoup/',\n '/source/software/BeautifulSoup/index.bhtml',\n '/self/',\n '/self/contact.html',\n 'http://creativecommons.org/licenses/by-sa/2.0/',\n 'http://creativecommons.org/licenses/by-sa/2.0/',\n 'http://www.crummy.com/',\n 'http://www.crummy.com/software/',\n 'http://www.crummy.com/software/BeautifulSoup/']"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# let's investigate. Have a close look at the link_list\n",
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n"
    }
   ],
   "source": [
    "# seems that there are None elements!\n",
    "# let's verify \n",
    "print(sum([l is None for l in link_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['https://code.launchpad.net/beautifulsoup',\n 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n 'http://lxml.de/',\n 'http://code.google.com/p/html5lib/',\n 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=enterprise',\n 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n 'https://bugs.launchpad.net/beautifulsoup/',\n 'https://tidelift.com/security',\n 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n 'http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html',\n 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup?utm_source=pypi-beautifulsoup&utm_medium=referral&utm_campaign=website',\n 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n 'https://github.com/BlankerL/DXY-COVID-19-Crawler',\n 'https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19',\n 'https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py',\n 'http://www.harrowell.org.uk/viktormap.html',\n 'http://svn.python.org/view/tracker/importer/',\n 'http://www2.ljworld.com/',\n 'http://www.b-list.org/weblog/2010/nov/02/news-done-broke/',\n 'http://esrl.noaa.gov/gsd/fab/',\n 'http://laps.noaa.gov/topograbber/',\n 'http://groups.google.com/group/beautifulsoup/',\n 'https://launchpad.net/beautifulsoup',\n 'https://code.launchpad.net/beautifulsoup/',\n 'https://bugs.launchpad.net/beautifulsoup/',\n 'http://creativecommons.org/licenses/by-sa/2.0/',\n 'http://creativecommons.org/licenses/by-sa/2.0/',\n 'http://www.crummy.com/',\n 'http://www.crummy.com/software/',\n 'http://www.crummy.com/software/BeautifulSoup/']"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# let's filter those objects out in the for loop\n",
    "external_links = []\n",
    "\n",
    "# write a loop to filter the links \n",
    "# if it is not None ans starts with 'http' it is ok\n",
    "for l in link_list:\n",
    "    if l is not None and l[:4] == 'http':\n",
    "        external_links.append(l)\n",
    "\n",
    "external_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**: The above `if` condition works because of lazy evaluation in Python. The `and` statement becomes `False` if the first part is `False`, so there is no need to ever evaluate the second part. Thus a `None` entry in the list gets never asked about its first four characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# redifining 's' without any line breaks\n",
    "s = \"\"\"<!DOCTYPE html><html><head><title>This is a title!</title></head><body><h3>Test!</h3><p>Hello World!</p></body></html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<head><title>This is a title!</title></head>\n<body><h3>Test!</h3><p>Hello World!</p></body>\n"
    }
   ],
   "source": [
    "# get bs4 object\n",
    "tree = bs4.BeautifulSoup(s)\n",
    "\n",
    "# get html root node \n",
    "root_node = tree.html\n",
    "\n",
    "# get head from root using contents \n",
    "head = root_node.contents[0]\n",
    "\n",
    "# get body from root \n",
    "body = root_node.contents[1]\n",
    "\n",
    "# could directly access body\n",
    "print(tree.head)\n",
    "print(tree.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions\n",
    "- find the h3 tag by parsing the tree starting at body\n",
    "- create a list of all **Hall of Fame** entries listed on the BeautifulSoup webpage \n",
    "    - hint: it is the only unordered list in the page (tag ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<h3>Test!</h3>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# get h3 tag from body\n",
    "body.contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<ul>\n<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n Type\"</a>, a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n\n<li>Jiabao Lin's <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">DXY-COVID-19-Crawler</a>\nuses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source: <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\"How open source software is fighting COVID-19\"</a>)\n\n<li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\na page that's been linked to and find a representative image</a>.\n\n<li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n activities</a> of an arms merchant.\n\n<li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\nbug tracker from Sourceforge to Roundup</a>.\n\n<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\nstatewide election results</a>.\n\n<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\ndownloading \"high resolution USGS datasets.\"\n\n</li></li></li></li></li></li></li></ul>\n"
    }
   ],
   "source": [
    "# use ul as entry point \n",
    "entry_point = soup.find('ul')\n",
    "print(entry_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n Type\"</a>, a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n\n<li>Jiabao Lin's <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">DXY-COVID-19-Crawler</a>\nuses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source: <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\"How open source software is fighting COVID-19\"</a>)\n\n<li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\na page that's been linked to and find a representative image</a>.\n\n<li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n activities</a> of an arms merchant.\n\n<li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\nbug tracker from Sourceforge to Roundup</a>.\n\n<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\nstatewide election results</a>.\n\n<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\ndownloading \"high resolution USGS datasets.\"\n\n</li></li></li></li></li></li></li>]\n"
    }
   ],
   "source": [
    "# get hall of fame list from entry point \n",
    "# skip the first entry\n",
    "hall_of_fame_list = entry_point.contents[1:]\n",
    "print(hall_of_fame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# reformat intro a list containing strings\n",
    "tmp = []\n",
    "for li in hall_of_fame_list:\n",
    "    tmp.append(li.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tmp now is actually a list of lists, containing the hall of fame entries. Here are some advanced Python on how to print really just one entry per list item:\n",
    "\n",
    "The cool things about this are:\n",
    "- the use of \"\" to just access the `join` function of strings\n",
    "- the `join` function itself\n",
    "- that you can actually have 2 nested `for` loops in a list comprehension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n Type\"</a>, a work of digital art on display in the lobby of the New\n York Times building, uses Beautiful Soup to scrape news feeds.\n\n<li>Jiabao Lin's <a href=\"https://github.com/BlankerL/DXY-COVID-19-Crawler\">DXY-COVID-19-Crawler</a>\nuses Beautiful Soup to scrape a Chinese medical site for information\nabout COVID-19, making it easier for researchers to track the spread\nof the virus. (Source: <a href=\"https://blog.tidelift.com/how-open-source-software-is-fighting-covid-19\">\"How open source software is fighting COVID-19\"</a>)\n\n<li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\na page that's been linked to and find a representative image</a>.\n\n<li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n activities</a> of an arms merchant.\n\n<li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\nbug tracker from Sourceforge to Roundup</a>.\n\n<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\nstatewide election results</a>.\n\n<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\ndownloading \"high resolution USGS datasets.\"\n\n</li></li></li></li></li></li>\n"
    }
   ],
   "source": [
    "test = [\"\".join(str(a) for a in sublist) for sublist in tmp]\n",
    "print('\\n'.join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to scrape the information from advertisements for data scientists from 'indeed.com'. Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "URL = 'https://jp.indeed.com/jobs?q=%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%83%86%E3%82%A3%E3%82%B9%E3%83%88'\n",
    "\n",
    "# conducting a request of the stated URL above\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ited{color:#fff}.icl-Button--primary:active,.icl-Button--primary:focus,.icl-Button--primary:hover,.icl-Button:active,.icl-Button:focus,.icl-Button:hover{color:#fff;text-decoration:none}[dir] .icl-Button--primary:active,[dir] .icl-Button--primary:focus,[dir] .icl-Button--primary:hover,[dir] .icl-Button:active,[dir] .icl-Button:focus,[dir] .icl-Button:hover{background-color:#1497ff;border-color:#1497ff}.icl-Button--primary:focus,.icl-Button:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7;outline:0}[dir] .icl-Button--primary:focus,[dir] .icl-Button:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7}.icl-Button--secondary{box-sizing:border-box;display:inline-block;color:#085ff7;-webkit-font-smoothing:antialiased;overflow:hidden;text-overflow:ellipsis;text-decoration:none;white-space:nowrap;-webkit-highlight:none;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-touch-callout:none;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[dir] .icl-Button--secondary{margin:0;text-align:center;background-color:#fff;border:.125rem solid #085ff7;border-radius:6.25rem;background-repeat:repeat-x;cursor:pointer}[dir] .icl-Button--secondary::-moz-focus-inner{border:0}.icl-Button--secondary:disabled,.icl-Button--secondary:disabled:hover{color:#ccc}[dir] .icl-Button--secondary:disabled,[dir] .icl-Button--secondary:disabled:hover{cursor:default;background:#fff;border:.125rem solid #ececec}.icl-Button--secondary:visited{color:#085ff7}.icl-Button--secondary:active,.icl-Button--secondary:focus,.icl-Button--secondary:hover{color:#1497ff;text-decoration:none}[dir] .icl-Button--secondary:active,[dir] .icl-Button--secondary:focus,[dir] .icl-Button--secondary:hover{background-color:#fff;border-color:#1497ff}.icl-Button--secondary:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7;outline:0}[dir] .icl-Button--secondary:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7}.icl-Button--tertiary{box-sizing:border-box;display:inline-block;color:#085ff7;-webkit-font-smoothing:antialiased;overflow:hidden;text-overflow:ellipsis;text-decoration:none;white-space:nowrap;-webkit-highlight:none;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-touch-callout:none;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[dir] .icl-Button--tertiary{margin:0;text-align:center;background-color:#fff;border:.125rem solid #ccc;border-radius:6.25rem;background-repeat:repeat-x;cursor:pointer}[dir] .icl-Button--tertiary::-moz-focus-inner{border:0}.icl-Button--tertiary:disabled,.icl-Button--tertiary:disabled:hover{color:#ccc}[dir] .icl-Button--tertiary:disabled,[dir] .icl-Button--tertiary:disabled:hover{cursor:default;background:#fff;border:.125rem solid #ececec}.icl-Button--tertiary:visited{color:#085ff7}.icl-Button--tertiary:active,.icl-Button--tertiary:focus,.icl-Button--tertiary:hover{color:#1497ff;text-decoration:none}[dir] .icl-Button--tertiary:active,[dir] .icl-Button--tertiary:focus,[dir] .icl-Button--tertiary:hover{background-color:#fff;border-color:#ccc}.icl-Button--tertiary:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7;outline:0}[dir] .icl-Button--tertiary:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7}.icl-Button--branded{box-sizing:border-box;display:inline-block;color:#fff;-webkit-font-smoothing:antialiased;overflow:hidden;text-overflow:ellipsis;text-decoration:none;white-space:nowrap;-webkit-highlight:none;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-touch-callout:none;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;font-size:1.1875rem}[dir] .icl-Button--branded{margin:0;text-align:center;background-color:#ff5a1f;border:.125rem solid #ff5a1f;border-radius:6.25rem;background-repeat:repeat-x;cursor:pointer}[dir] .icl-Button--branded::-moz-focus-inner{border:0}.icl-Button--branded:disabled,.icl-Button--branded:disabled:hover{color:#ccc}[dir] .icl-Button--branded:disabled,[dir] .icl-Button--branded:disabled:hover{cursor:default;background:#fff;border:.125rem solid #ececec}.icl-Button--branded:visited{color:#fff}.icl-Button--branded:active,.icl-Button--branded:focus,.icl-Button--branded:hover{color:#fff;text-decoration:none}[dir] .icl-Button--branded:active,[dir] .icl-Button--branded:focus,[dir] .icl-Button--branded:hover{background-color:#ff6c40;border-color:#ff6c40}.icl-Button--branded:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #ff5a1f;outline:0}[dir] .icl-Button--branded:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #ff5a1f}.icl-Button--block{display:block;width:100%;max-width:21.9375rem}[dir=ltr] .icl-Button--block,[dir=rtl] .icl-Button--block{margin-left:auto;margin-right:auto}[dir=ltr] .icl-Button--icon,[dir=rtl] .icl-Button--icon{padding-left:10px;padding-right:10px}.icl-Button--responsive{max-width:21.9375rem;width:100%}[dir] .icl-Button--responsive:first-child{margin-top:0}@media only screen and (min-width:768px){.icl-Button--responsive{width:auto}}.icl-Button--special{box-sizing:border-box;display:inline-block;color:#fff;-webkit-font-smoothing:antialiased;overflow:hidden;text-overflow:ellipsis;text-decoration:none;white-space:nowrap;-webkit-highlight:none;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-touch-callout:none;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[dir] .icl-Button--special{margin:0;text-align:center;background-color:#f60;border:.125rem solid #f60;border-radius:6.25rem;background-repeat:repeat-x;cursor:pointer}[dir] .icl-Button--special::-moz-focus-inner{border:0}.icl-Button--special:disabled,.icl-Button--special:disabled:hover{color:#ccc}[dir] .icl-Button--special:disabled,[dir] .icl-Button--special:disabled:hover{cursor:default;background:#fff;border:.125rem solid #ececec}.icl-Button--special:visited{color:#fff}.icl-Button--special:active,.icl-Button--special:focus,.icl-Button--special:hover{color:#fff;text-decoration:none}[dir] .icl-Button--special:active,[dir] .icl-Button--special:focus,[dir] .icl-Button--special:hover{background-color:#f60;border-color:#f60}.icl-Button--special:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7;outline:0}[dir] .icl-Button--special:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7}.icl-Button--danger{box-sizing:border-box;display:inline-block;color:#fff;-webkit-font-smoothing:antialiased;overflow:hidden;text-overflow:ellipsis;text-decoration:none;white-space:nowrap;-webkit-highlight:none;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-touch-callout:none;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[dir] .icl-Button--danger{margin:0;text-align:center;background-color:#db183f;border:.125rem solid #db183f;border-radius:6.25rem;background-repeat:repeat-x;cursor:pointer}[dir] .icl-Button--danger::-moz-focus-inner{border:0}.icl-Button--danger:disabled,.icl-Button--danger:disabled:hover{color:#ccc}[dir] .icl-Button--danger:disabled,[dir] .icl-Button--danger:disabled:hover{cursor:default;background:#fff;border:.125rem solid #ececec}.icl-Button--danger:visited{color:#fff}.icl-Button--danger:active,.icl-Button--danger:focus,.icl-Button--danger:hover{color:#fff;text-decoration:none}[dir] .icl-Button--danger:active,[dir] .icl-Button--danger:focus,[dir] .icl-Button--danger:hover{background-color:#db183f;border-color:#db183f}.icl-Button--danger:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #db183f;outline:0}[dir] .icl-Button--danger:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #db183f}.icl-Button--working{box-sizing:border-box;display:inline-block;color:#fff;-webkit-font-smoothing:antialiased;overflow:hidden;text-overflow:ellipsis;text-decoration:none;white-space:nowrap;-webkit-highlight:none;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-touch-callout:none;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[dir] .icl-Button--working{margin:0;text-align:center;background-color:#085ff7;border:.125rem solid #085ff7;border-radius:6.25rem;background-repeat:repeat-x;cursor:pointer}[dir] .icl-Button--working::-moz-focus-inner{border:0}.icl-Button--working:disabled,.icl-Button--working:disabled:hover{color:#ccc}[dir] .icl-Button--working:disabled,[dir] .icl-Button--working:disabled:hover{cursor:default;background:#fff;border:.125rem solid #ececec}.icl-Button--working:visited{color:#fff}.icl-Button--working:active,.icl-Button--working:focus,.icl-Button--working:hover{color:#fff;text-decoration:none}[dir] .icl-Button--working:active,[dir] .icl-Button--working:focus,[dir] .icl-Button--working:hover{background-color:#1497ff;border-color:#1497ff}.icl-Button--working:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7;outline:0}[dir] .icl-Button--working:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7}.icl-Button--transparent{box-sizing:border-box;display:inline-block;color:#085ff7;-webkit-font-smoothing:antialiased;overflow:hidden;text-overflow:ellipsis;text-decoration:none;white-space:nowrap;-webkit-highlight:none;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-touch-callout:none;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[dir] .icl-Button--transparent{margin:0;text-align:center;background-color:transparent;border:.125rem solid transparent;border-radius:6.25rem;background-repeat:repeat-x;cursor:pointer}[dir] .icl-Button--transparent::-moz-focus-inner{border:0}.icl-Button--transparent:disabled,.icl-Button--transparent:disabled:hover{color:#ccc}[dir] .icl-Button--transparent:disabled,[dir] .icl-Button--transparent:disabled:hover{cursor:default;background:#fff;border:.125rem solid #ececec}.icl-Button--transparent:visited{color:#085ff7}.icl-Button--transparent:active,.icl-Button--transparent:focus,.icl-Button--transparent:hover{color:#085ff7;text-decoration:none}[dir] .icl-Button--transparent:active,[dir] .icl-Button--transparent:focus,[dir] .icl-Button--transparent:hover{background-color:transparent;border-color:transparent}.icl-Button--transparent:focus{-webkit-box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7;outline:0}[dir] .icl-Button--transparent:focus{box-shadow:0 0 0 2px #fff,0 0 0 4px #085ff7}\n/*# sourceMappingURL=Button.css.map*/\n        </style>\n        <div id=\"resumeCtaFooter\" style=\"height:64px;\">\n         <div style=\"position:absolute;width:100%;\">\n          <style type=\"text/css\">\n           .footerCta {\n                        text-align:center;\n                        margin:0px;\n                        font-size:15px;\n                        width:100%;\n                        background-color:#ebebeb;\n                        color:#000000;\n                    }\n\n                    div.content >table {\n                        margin-bottom: 4em;\n                    }\n          </style>\n          <script type=\"text/javascript\">\n           if(null !== call_when_jsall_loaded) {call_when_jsall_loaded(function() {if(!!window.logPromoImpression) {window.logPromoImpression('trk.origin=jobsearch&trk.variant=FooterGrayBelow&trk.pos=below&trk.tk=1e6e51sod0uk7000', 'resume');}})}\n          </script>\n          <div class=\"footerCta greyBar\">\n           求人に簡単応募\n           <style type=\"text/css\">\n            .cta_button {\n            text-decoration:none !important;\n            margin: 12px !important;\n        }\n\n        .cta_button.blue  {\n            color: #f8f8f9 !important;\n        }\n\n        .cta_button.grey {\n            color: #000000 !important;\n        }\n\n        .cta_button.uploaded {\n            border-radius: 100px !important;\n            font-size: 12px;\n            line-height: 16px;\n            text-align: center;\n            padding: 8px;\n        }\n\n        .cta_button.continue {\n            color: #085ff7 !important;\n            background-color: #ffffff !important;\n            border: 2px solid #195ff7 !important;\n            width: 163px;\n        }\n\n        .cta_button.accept {\n            color: #ffffff !important;\n            background-color: #085ff7 !important;\n            border: 2px solid #085ff7 !important;\n            width: 106px;\n            display: inline-block;\n        }\n\n        .cta_button.back {\n            color: #085ff7 !important;\n            background-color: #ffffff !important;\n            border: 2px solid #cccccc !important;\n            width: 106px;\n            margin-left: -4px !important;\n        }\n           </style>\n           <span dir=\"ltr\">\n            <a class=\"icl-Button icl-Button--primary icl-Button--sm cta_button blue\" href=\"/promo/resume?from=bottomResumeCTAjobsearch&amp;trk.origin=jobsearch\" onclick=\"if(!!window.logPromoClick) {window.logPromoClick('trk.origin=jobsearch&amp;trk.variant=FooterGrayBelow&amp;trk.pos=below&amp;trk.tk=1e6e51sod0uk7000', 'resume','/promo/resume?from=bottomResumeCTAjobsearch&amp;trk.origin=jobsearch');}\">\n             履歴書を作成する\n            </a>\n           </span>\n          </div>\n         </div>\n        </div>\n       </div>\n      </div>\n     </td>\n    </tr>\n   </tbody>\n  </table>\n  <script type=\"text/javascript\">\n   <!--\n\nlogJSVPing('jsv', '1e6e51sod0uk7000');\nfunction jsall_loaded() {\n\n\ninitProcessLeftoverDwellEntries();\n\ndetectBrowserState('jobsearch', '1e6e51sod0uk7000');\n\ninitLogTiming('jobsearch', '1e6e51sod0uk7000');\nattachSjBlock('');\nattachJaBlock('');\n}\nif (window['closureReady'] === true) {\njsall_loaded();\n}\n//-->\n  </script>\n  <script type=\"text/javascript\">\n   PENDING_ANALYTICS_VARS = window.PENDING_ANALYTICS_VARS || [];\nPENDING_ANALYTICS_VARS[PENDING_ANALYTICS_VARS.length] = ['_setCustomVar', 5, 'loggedIn', 'false', 3];\n  </script>\n  <script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=DC-8232301\">\n  </script>\n  <script>\n   window.dataLayer = window.dataLayer || [];\nfunction gtag(){dataLayer.push(arguments);}\ngtag('js', new Date());\ngtag('config', 'DC-8232301');\n  </script>\n  <script type=\"text/javascript\">\n   var ga_domains = [];\nga_domains.push('indeed.co.in');ga_domains.push('indeed.lu');ga_domains.push('indeed.fr');ga_domains.push('indeed.de');ga_domains.push('indeed.com.br');ga_domains.push('indeed.co.uk');ga_domains.push('indeed.hk');ga_domains.push('indeed.fi');ga_domains.push('indeed.pt');ga_domains.push('indeed.jp');ga_domains.push('indeed.com');ga_domains.push('indeed.com.sg');ga_domains.push('indeed.nl');ga_domains.push('indeed.com.pk');ga_domains.push('indeed.cl');ga_domains.push('indeed.es');ga_domains.push('indeed.co.ve');ga_domains.push('indeed.ae');ga_domains.push('indeed.com.mx');ga_domains.push('indeed.com.my');ga_domains.push('indeed.ch');ga_domains.push('indeed.com.co');ga_domains.push('indeed.com.ph');ga_domains.push('indeed.co.za');ga_domains.push('indeed.ie');ga_domains.push('indeed.com.au');ga_domains.push('indeed.ca');ga_domains.push('indeed.com.pe');\n\n(function (i, s, o, g, r, a, m) {\ni['GoogleAnalyticsObject'] = r;\ni[r] = i[r] || function () {\n(i[r].q = i[r].q || []).push(arguments)\n}, i[r].l = 1 * new Date();\na = s.createElement(o),\nm = s.getElementsByTagName(o)[0];\na.async = 1;\na.src = g;\nm.parentNode.insertBefore(a, m)\n})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');\n\nvar ga = ga || [];\nga('create', 'UA-90780-1', 'auto', {\n'allowLinker': true\n});\nga('require', 'linkid');\nga('require', 'linker');\nga('linker:autoLink', ga_domains, false, true);\nga('require', 'displayfeatures');\nga('send', 'pageview');\n\n(function () {\nif (window.PENDING_ANALYTICS_VARS && window.PENDING_ANALYTICS_VARS.length > 0) {\nfor (var i in PENDING_ANALYTICS_VARS) {\nga('set', PENDING_ANALYTICS_VARS[i][2], PENDING_ANALYTICS_VARS[i][3]);\n}\n}\n})();\n  </script>\n  <script>\n   window._comscore = window._comscore || [];\nwindow._comscore.push({ c1: \"2\", c2: \"6486505\", c4:\"jp.indeed.com/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%83%86%E3%82%A3%E3%82%B9%E3%83%88%E9%96%A2%E9%80%A3%E3%81%AE%E6%B1%82%E4%BA%BA\", c15:\"1e6e51sbl0uk7000\"});\n(function() { var s = document.createElement(\"script\"), el = document.getElementsByTagName(\"script\")[0]; s.async = true; s.src = (document.location.protocol == \"https:\" ? \"https://sb\" : \"http://b\") + \".scorecardresearch.com/beacon.js\"; el.parentNode.insertBefore(s, el); })();\n  </script>\n  <noscript>\n   <img alt=\"\" height=\"0\" src=\"https://sb.scorecardresearch.com/p?c1=2&amp;c2=6486505&amp;c4=jp.indeed.com%2F%25E3%2583%2587%25E3%2583%25BC%25E3%2582%25BF%25E3%2582%25B5%25E3%2582%25A4%25E3%2582%25A8%25E3%2583%25B3%25E3%2583%2586%25E3%2582%25A3%25E3%2582%25B9%25E3%2583%2588%25E9%2596%25A2%25E9%2580%25A3%25E3%2581%25AE%25E6%25B1%2582%25E4%25BA%25BA&amp;c15=1e6e51sbl0uk7000&amp;cv=2.0&amp;cj=1\" style=\"display:none\" width=\"0\"/>\n  </noscript>\n  <script id=\"mosaic-init-data\" type=\"text/javascript\">\n   window.mosaic= window.mosaic || {};\n    window.mosaic.providers={\"mosaic-provider-reportcontent\":\"https://d3fw5vlhllyvee.cloudfront.net/mosaic-provider-reportcontent/dist/974f40c2c6286e448556/js/ReportContent-client.js\",\"platformJs\":\"https://d3fw5vlhllyvee.cloudfront.net/mosaic-common/dist/ad7c6f4b0f1312af80ff/scripts/index.js\"};\n    window.mosaic.providerErrors={};\n    window.mosaic.provideri18N={\"mosaic-provider-reportcontent\":\"https://d3fw5vlhllyvee.cloudfront.net/mosaic-provider-reportcontent/dist/i18n/f781537ffa965c988557/ja.js\"};\n    window.mosaic.lazyProviders={\"mosaic-provider-reportcontent\":\"\\u003cdiv class\\u003d\\\"reportcontent-injection-wrapper\\\"\\u003e\\u003c/div\\u003e\"};\n    window.mosaic.cssResetProviders={\"mosaic-provider-reportcontent\":false}\n    window.mosaic.zonedProviders={\"salaryFilter\":[],\"afterTenthJobResult\":[],\"belowPageContent\":[],\"aboveJobCards\":[]}\n    window.mosaic.serviceIdLookup={\"mosaic-provider-reportcontent\":\"mosaic-provider-reportcontent\"}\n    window.mosaic.webpackPublicPath=\"https://d3fw5vlhllyvee.cloudfront.net/mosaic-common/dist\"\n  </script>\n  <script id=\"mosaic-data\" type=\"text/javascript\">\n   window.mosaic.providerData={};\n    window.mosaic.initialData = {\"logTypes\":{\"jsuipPlatformInitError\":\"4327faee2bbb67f4e7f2185701a52e54\",\"mosaicPlatformLoadRecovery\":\"cee115a2f7013215fe1cde93caa21161\",\"mosaicProviderSeen\":\"e3926cc0a343f384f2e5e6dfc540016a\",\"jsuipProviderLoadTimings\":\"fb7274d51fc7a04d450086c83a40cc6a\",\"jsuipProviderExecutionError\":\"211e778edbde5e164fd5f14dd23e6115\",\"jsuipAction\":\"d5973eb3ad332b3608839e9bbed7d867\",\"jsuipPlatformLoadError\":\"c74c8d65f1d6f620ddce517edd3faa40\",\"jsuipProviderLoadError\":\"61c37c1721757c82be5e97b1fff8d1c2\"},\"platformLogTk\":\"1e6e51soh0uk7000\",\"logTk\":\"1e6e51sod0uk7000\",\"hostId\":\"jasx\",\"platformId\":\"jasx\",\"env\":\"PRODUCTION\",\"pageId\":\"serp\"};\n\n    window.mosaic.providerData[\"mosaic-provider-reportcontent\"]={\"hostId\":\"jasx\",\"pageId\":\"serp\",\"ctk\":\"1e6e51sbl0uk7000\",\"country\":\"JP\",\"language\":\"ja\",\"userAgent\":\"python-requests/2.23.0\",\"mobvjtk\":\"1e6e51sod0uk7000\",\"isMobile\":false,\"indeedcsrftoken\":\"ykJwzB9v5CVmINkcv7pPee6NaMutFDo7\",\"isLoggedIn\":false,\"isConfirmed\":true,\"reportContentApiUrl\":\"https://reportcontent.indeed.com/api/v1/report/content\"};\n  </script>\n  <script async=\"\" src=\"https://d3fw5vlhllyvee.cloudfront.net/mosaic-common/dist/7cfe1d15604534cea6bb/scripts/loadProviders.js\">\n  </script>\n  <script id=\"mosaic-translation-overrides\">\n   this.mosaic = this.mosaic || {}; this.mosaic.i18nOverrides = this.mosaic.i18nOverrides || {};\n  </script>\n  <script>\n   window['sendPageLoadEndPing'] = function(pageId, tk, st) {var validPageIds = ['viewjob', 'serp']; if (!!Image && validPageIds.indexOf(pageId) > -1 && !!tk && !!st) {var href = '/rpc/pageLoadEnd?pageId=' + pageId + '&tk=' + tk + '&st=' + st + '&__=' + Math.random(); var img = new Image(); img.src = href;}}; window['sendPageLoadEndPing'](\"serp\", \"1e6e51sod0uk7000\", \"1587465483021\");\n  </script>\n </body>\n</html>\n\n"
    }
   ],
   "source": [
    "# specifying a desired format pf 'page' using\n",
    "# the html parser. This allows Python to read\n",
    "# the various components of the page, rather \n",
    "# than treating it as one long string\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Withdrawing basic elements of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['データサイエンティスト / Data Labs',\n 'データサイエンティスト(2022年度新卒採用)\\u3000',\n 'データサイエンティスト',\n 'データサイエンティスト / データ分析 / データ・ドリブン施策提案',\n '研究開発分野 募集ポジション',\n 'データサイエンティスト・データアナリスト',\n 'データサイエンティスト(データベース・スペシャリスト) / Data Scientist (Database Specialist)',\n 'データサイエンティスト/データアナリスト(データ分析)',\n 'データサイエンティスト',\n 'データサイエンティスト',\n 'AI/データ領域のソリューション営業',\n 'データサイエンティスト',\n 'データサイエンティスト',\n 'データサイエンティスト\\xa0',\n 'データサイエンティスト、データ分析官(キャリアチェンジ)',\n '未経験歓迎lデータサイエンティスト']"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "def extract_job_title_from_result(soup):\n",
    "    jobs = []\n",
    "    \n",
    "    for div in soup.find_all(name='div', attrs={\"class\":\"row\"}): \n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "            jobs.append(a[\"title\"])\n",
    "    return(jobs)\n",
    "\n",
    "extract_job_title_from_result(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Perfect! All 16 jobs are listed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Company name\n",
    "Company names are a bit tricky, as most would appear in `<span>` tags, with \"class\":\"company\". Rarely, however they will be housed in `<span>` tags with \"class\":\"result-link-source\".\n",
    "\n",
    "The `if/else` statement hepls to extract the company info either of these places. Comapny names are output with a lot of white space around them, so inputting `.strip()` at the end helps to remove this when extracting the info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['LINE株式会社',\n 'パーソルキャリア株式会社',\n '株式会社ブレインパッド',\n '株式会社レッジ',\n '株式会社資生堂',\n 'L2株式会社',\n '中外製薬グループ',\n 'コグラフ株式会社',\n '株式会社ジーユーエヌ',\n '社会福祉法人善光会',\n '株式会社エイアイ・フィールド(AIフィールド)',\n '合同会社DILIGENCE',\n 'キヤノンITソリューションズ株式会社',\n '株式会社aiforce solutions',\n '株式会社エイアイ・フィールド(AIフィールド)',\n '株式会社エイアイ・フィールド(AIフィールド)']"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "def extract_company_from_result(soup):\n",
    "    companies = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    return(companies)\n",
    "\n",
    "extract_company_from_result(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Location\n",
    "Locations are located unfer the `<span>` tags. Span tags are sometimes nested within each other, such that the location text may sometimes be within \"class\":\"location\" attributes, or nested in \"itemprop\":\"\"addressLocality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['新宿区 新宿',\n '千代田区 大手町',\n '港区 白金台',\n '品川区 五反田駅',\n '千代田区',\n '千代田区',\n '横浜市 西区',\n '港区 表参道駅',\n '中央区',\n '品川区 五反田駅',\n '新宿区 新宿',\n '港区',\n '品川区 五反田駅',\n '渋谷区 神宮前',\n '港区 三田',\n '千代田区 大手町']"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "def extract_location_from_result(soup):\n",
    "    locations = []\n",
    "    spans = soup.findAll('span', attrs={'class': 'location'})\n",
    "    divs = soup.findAll('div', attrs={'class': 'location'})\n",
    "    \n",
    "    for div in divs:\n",
    "        locations.append(div.text)\n",
    "\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "    \n",
    "    return(locations)\n",
    "\n",
    "extract_location_from_result(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Salary\n",
    "It is difficult to extract 'Salary' info from job postings. Most postings don't contain any salary info at all.\n",
    "\n",
    "Some salaries are housed under the `<nobr>` tags, while others are under `<div>` tags, \"class\":\"sjcl\" and are under seperate nested `<div>` tags with no attributes. `try/except` statements aree particularly helpful in withdrawing this info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['年収 350万 ~ 450万円',\n '月給 34万 ~ 84万円',\n '月給 42万 ~ 84万円',\n '月給 25万 ~ 60万円',\n '年収 350万 ~ 800万円',\n '月給 27万 ~ 80万円',\n '年収 400万 ~ 700万円',\n '年収 350万 ~ 700万円']"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "def extract_salary_from_result(soup):\n",
    "    salaries = []\n",
    "    spans = soup.findAll('span', attrs={'class':'salaryText'})\n",
    "    for span in spans:\n",
    "        salaries.append(span.text.strip())\n",
    "    return(salaries)\n",
    "\n",
    "extract_salary_from_result(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['や利用動向などのデータを横断的に処理し、 より精密なデータ分析と情報フィルタリングを提供し、全サービスのデータの効率的な... 価値の最大化をデータ分析によって実現できるデータ分析者を募集...',\n 'データツールの開発 * 分析プラットフォーム開発 データ... 関連キーワード: データアナリスト・データサイエンティスト・データ分析エンジニア・ビッグデータ解析データスペシャリスト...',\n '【職種名】 データサイエンティスト 【勤務地】 白金台... ます。 深層学習や機械学習関連の文献調査・実装を行い、データサイエンティストや コンサルタントと連携しながら、ソリューシ...',\n 'データ・ドリブン施策提案などの業務をお願いいたします。 ・Webログデータ、事業データのデータ分析業務 ・データ/ドリブ... 経験 ・データアナリスト、データサイエンティスト、データエン...',\n '心理生理学研究 • 美容機器開発 • 化粧品容器・用具開発 • AI技術者 • データサイエンティスト • Smartphone向けアプリエンジニア 化粧品の安全性評価 全身毒性...',\n '析に必要なデータ集計及び最適なツールの運用 ・広告効果や位置情報などのデータ解析や仮説立案 ・データ解析を軸としたシステ... データ分析、データマイニング業務経験 ・顧客情報のデータ管理...',\n '提供する。 ・データベース上にあるデータに対して、解析技術者とともに必要なデータセットを定義し、解析技術者が必要なデータ... 扱うデータもオミクスデータ・製造データ、リアルワールドデータ...',\n '日時など分析に必要なデータを確認） ③データ抽出作業（SQL... 伴い、データサイエンティスト／データアナリストとして 一緒に会社を成長させていく仲間を募集しています。 配属部署 データ...',\n 'データサイエンティスト 職務内容 データの分析業務 求めるスキル データ分析会社、シンクタンク、事業会社でのデータ分析業務経験 データ分析関連知識 Python、R、SQL、SAS...',\n '【選考方法】 書類選考→面接(1~2回)',\n 'リューション営業/AI/データ/機械学習/データ分析/DMP/提案営業 一言 営業職の募集です。 AI・データサイエンス... 案内容：AI/データサイエンス領域 - データ系人材（派遣...',\n '経験者募集】データサイエンティスト、データアナリスト ※報酬還元率70% 【仕事内容】 ビッグデータ、データマイニング... R、ABAP、PHP\\u3000他 •データベース:MySQL...',\n '募集職種 業務内容 データ分析、機械学習の知識を持ちデータから価値を創... 進する組織に配属し、様々な業種のデータに触れることが出来ます。 データサイエンティストとしての経験、スキルを熱意を持って...',\n 'データサイエンティスト AIベンチャーの中核となるデータサイエンティスト ＜募集要項＞ ＜必要業務経験＞ ＜歓迎経... を利用してデータ解析（3年以上） ・ビッグデータを統計言語...',\n 'データサイエンティスト、データ分析官（キャリアチェンジ） 募集職種 データサイエンティスト/機械学習エンジニア/データ... 務内容 ・データサイエンティスト領域 1. データマイニ...',\n 'lデータサイエンティスト 募集職種 データサイエンティスト... 業務内容 ・データサイエンティスト領域 1. データマイニング 2. データベース・マーケティング 3. データ...']"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "def extract_summary_from_result(soup):\n",
    "    summaries = []\n",
    "    divs = soup.findAll('div', attrs={'class':'summary'})\n",
    "    for div in divs:\n",
    "        summaries.append(div.text.replace('\\n', ''))\n",
    "    return(summaries)\n",
    "\n",
    "extract_summary_from_result(soup)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda62b3a3efb99e4e62a97c3134338cdf29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}